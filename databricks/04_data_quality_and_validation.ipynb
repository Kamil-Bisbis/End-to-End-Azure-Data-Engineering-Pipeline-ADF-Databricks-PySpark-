{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04 Data Quality and Validation\n",
        "\n",
        "This notebook runs lightweight data quality checks across Bronze, Silver, and Gold layers.\n",
        "\n",
        "Checks included:\n",
        "- Row counts (and optional drop threshold)\n",
        "- Schema comparison (column set + type mismatches)\n",
        "- Null checks on required columns\n",
        "- Duplicate checks on primary keys\n",
        "- Optional distinct-PK reconciliation between layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DEFAULT_FORMAT = \"delta\"  # delta, parquet, csv\n",
        "\n",
        "# A conservative threshold to avoid false failures.\n",
        "# Example: if silver count drops more than 30% vs bronze, flag it.\n",
        "MAX_ROW_DROP_PCT = 0.30\n",
        "\n",
        "# If true, the notebook raises an exception when failures are found.\n",
        "# (In Databricks jobs, this can fail the task.)\n",
        "FAIL_PIPELINE_ON_ERROR = False\n",
        "\n",
        "# Datasets to validate.\n",
        "# Fill in paths that match your project.\n",
        "# If Gold is an aggregated table without a stable PK, set pk_cols=[] and compare_pk=False.\n",
        "\n",
        "DATASETS = [\n",
        "    {\n",
        "        \"name\": \"customers\",\n",
        "        \"format\": DEFAULT_FORMAT,\n",
        "        \"bronze_path\": \"/mnt/your_mount/bronze/customers\",\n",
        "        \"silver_path\": \"/mnt/your_mount/silver/customers\",\n",
        "        \"gold_path\": \"/mnt/your_mount/gold/customers\",\n",
        "        \"pk_cols\": [\"CustomerID\"],\n",
        "        \"not_null_cols\": [\"CustomerID\"],\n",
        "        \"compare_pk\": True\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"sales\",\n",
        "        \"format\": DEFAULT_FORMAT,\n",
        "        \"bronze_path\": \"/mnt/your_mount/bronze/sales\",\n",
        "        \"silver_path\": \"/mnt/your_mount/silver/sales\",\n",
        "        \"gold_path\": \"/mnt/your_mount/gold/sales\",\n",
        "        \"pk_cols\": [\"SalesOrderID\", \"SalesOrderDetailID\"],\n",
        "        \"not_null_cols\": [\"SalesOrderID\"],\n",
        "        \"compare_pk\": True\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"products\",\n",
        "        \"format\": DEFAULT_FORMAT,\n",
        "        \"bronze_path\": \"/mnt/your_mount/bronze/products\",\n",
        "        \"silver_path\": \"/mnt/your_mount/silver/products\",\n",
        "        \"gold_path\": \"/mnt/your_mount/gold/products\",\n",
        "        \"pk_cols\": [\"ProductID\"],\n",
        "        \"not_null_cols\": [\"ProductID\"],\n",
        "        \"compare_pk\": True\n",
        "    }\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# HELPERS\n",
        "# =========================\n",
        "\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "def _safe_display(obj):\n",
        "    \"\"\"Use Databricks display() if available, else fallback to print.\"\"\"\n",
        "    try:\n",
        "        display(obj)  # type: ignore\n",
        "    except Exception:\n",
        "        print(obj)\n",
        "\n",
        "def load_df(path: str, fmt: str):\n",
        "    \"\"\"Load a Spark DataFrame from a path.\"\"\"\n",
        "    if fmt.lower() == \"delta\":\n",
        "        return spark.read.format(\"delta\").load(path)\n",
        "    if fmt.lower() == \"parquet\":\n",
        "        return spark.read.parquet(path)\n",
        "    if fmt.lower() == \"csv\":\n",
        "        return spark.read.option(\"header\", \"true\").csv(path)\n",
        "    raise ValueError(f\"Unsupported format: {fmt}\")\n",
        "\n",
        "def exists_path(path: str) -> bool:\n",
        "    \"\"\"Check whether a path exists in Databricks FS / mounted storage.\"\"\"\n",
        "    try:\n",
        "        dbutils.fs.ls(path)  # type: ignore\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def schema_map(df) -> Dict[str, str]:\n",
        "    return {f.name: f.dataType.simpleString() for f in df.schema.fields}\n",
        "\n",
        "def count_nulls(df, cols: List[str]) -> Dict[str, int]:\n",
        "    from pyspark.sql import functions as F\n",
        "    out = {}\n",
        "    for c in cols:\n",
        "        if c not in df.columns:\n",
        "            out[c] = -1  # missing column\n",
        "        else:\n",
        "            out[c] = df.filter(F.col(c).isNull()).count()\n",
        "    return out\n",
        "\n",
        "def count_duplicates(df, pk_cols: List[str]) -> int:\n",
        "    if not pk_cols:\n",
        "        return 0\n",
        "    from pyspark.sql import functions as F\n",
        "    for c in pk_cols:\n",
        "        if c not in df.columns:\n",
        "            return -1  # missing pk col\n",
        "    dup = (\n",
        "        df.groupBy(*pk_cols)\n",
        "          .agg(F.count(\"*\").alias(\"n\"))\n",
        "          .filter(F.col(\"n\") > 1)\n",
        "          .count()\n",
        "    )\n",
        "    return dup\n",
        "\n",
        "def distinct_pk_count(df, pk_cols: List[str]) -> int:\n",
        "    if not pk_cols:\n",
        "        return 0\n",
        "    for c in pk_cols:\n",
        "        if c not in df.columns:\n",
        "            return -1\n",
        "    return df.select(*pk_cols).distinct().count()\n",
        "\n",
        "def pct_drop(a: int, b: int) -> float:\n",
        "    \"\"\"Percent drop from a -> b. If a=0, return 0.\"\"\"\n",
        "    if a <= 0:\n",
        "        return 0.0\n",
        "    return max(0.0, (a - b) / float(a))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# RUN VALIDATION\n",
        "# =========================\n",
        "\n",
        "results: List[Dict[str, Any]] = []\n",
        "failures: List[str] = []\n",
        "\n",
        "for ds in DATASETS:\n",
        "    name = ds[\"name\"]\n",
        "    fmt = ds.get(\"format\", DEFAULT_FORMAT)\n",
        "    pk_cols = ds.get(\"pk_cols\", [])\n",
        "    not_null_cols = ds.get(\"not_null_cols\", [])\n",
        "    compare_pk = bool(ds.get(\"compare_pk\", False))\n",
        "\n",
        "    layer_info = {\n",
        "        \"bronze\": ds.get(\"bronze_path\"),\n",
        "        \"silver\": ds.get(\"silver_path\"),\n",
        "        \"gold\": ds.get(\"gold_path\"),\n",
        "    }\n",
        "\n",
        "dfs: Dict[str, Any] = {}\n",
        "counts: Dict[str, int] = {}\n",
        "schemas: Dict[str, Dict[str, str]] = {}\n",
        "nulls: Dict[str, Dict[str, int]] = {}\n",
        "dups: Dict[str, int] = {}\n",
        "distinct_pk: Dict[str, int] = {}\n",
        "\n",
        "    # Load each layer if it exists\n",
        "    for layer, path in layer_info.items():\n",
        "        if not path:\n",
        "            continue\n",
        "        if not exists_path(path):\n",
        "            results.append({\n",
        "                \"dataset\": name,\n",
        "                \"layer\": layer,\n",
        "                \"status\": \"MISSING_PATH\",\n",
        "                \"path\": path\n",
        "            })\n",
        "            continue\n",
        "        try:\n",
        "            df = load_df(path, fmt)\n",
        "            dfs[layer] = df\n",
        "            counts[layer] = df.count()\n",
        "            schemas[layer] = schema_map(df)\n",
        "            nulls[layer] = count_nulls(df, not_null_cols)\n",
        "            dups[layer] = count_duplicates(df, pk_cols)\n",
        "            distinct_pk[layer] = distinct_pk_count(df, pk_cols) if compare_pk else 0\n",
        "        except Exception as e:\n",
        "            results.append({\n",
        "                \"dataset\": name,\n",
        "                \"layer\": layer,\n",
        "                \"status\": \"LOAD_ERROR\",\n",
        "                \"path\": path,\n",
        "                \"error\": str(e)[:250]\n",
        "            })\n",
        "\n",
        "    # Summary rows per layer\n",
        "    for layer in [\"bronze\", \"silver\", \"gold\"]:\n",
        "        if layer not in dfs:\n",
        "            continue\n",
        "        row = {\n",
        "            \"dataset\": name,\n",
        "            \"layer\": layer,\n",
        "            \"status\": \"LOADED\",\n",
        "            \"row_count\": counts.get(layer, None),\n",
        "            \"dup_pk_groups\": dups.get(layer, None),\n",
        "            \"distinct_pk\": distinct_pk.get(layer, None),\n",
        "            \"missing_not_null_cols\": \",\".join([c for c, v in nulls.get(layer, {}).items() if v == -1]),\n",
        "            \"not_null_violations\": \",\".join([f\"{c}:{v}\" for c, v in nulls.get(layer, {}).items() if v not in (-1, 0)]),\n",
        "        }\n",
        "        results.append(row)\n",
        "\n",
        "    # Cross-layer checks\n",
        "    if \"bronze\" in dfs and \"silver\" in dfs:\n",
        "        drop = pct_drop(counts[\"bronze\"], counts[\"silver\"])\n",
        "        if drop > MAX_ROW_DROP_PCT:\n",
        "            failures.append(\n",
        "                f\"{name}: bronze->silver row drop {drop:.1%} exceeds threshold {MAX_ROW_DROP_PCT:.1%}\"\n",
        "            )\n",
        "\n",
        "        bronze_cols = set(schemas[\"bronze\"].keys())\n",
        "        silver_cols = set(schemas[\"silver\"].keys())\n",
        "\n",
        "        missing_in_silver = sorted(list(bronze_cols - silver_cols))\n",
        "        if missing_in_silver:\n",
        "            results.append({\n",
        "                \"dataset\": name,\n",
        "                \"layer\": \"bronze_vs_silver\",\n",
        "                \"status\": \"SCHEMA_WARN\",\n",
        "                \"detail\": f\"Missing in silver (sample): {missing_in_silver[:20]}\" + (\" ...\" if len(missing_in_silver) > 20 else \"\")\n",
        "            })\n",
        "\n",
        "        common = sorted(list(bronze_cols & silver_cols))\n",
        "        type_mismatches = [\n",
        "            c for c in common\n",
        "            if schemas[\"bronze\"].get(c) != schemas[\"silver\"].get(c)\n",
        "        ]\n",
        "        if type_mismatches:\n",
        "            results.append({\n",
        "                \"dataset\": name,\n",
        "                \"layer\": \"bronze_vs_silver\",\n",
        "                \"status\": \"TYPE_WARN\",\n",
        "                \"detail\": f\"Type mismatches (sample): {type_mismatches[:20]}\" + (\" ...\" if len(type_mismatches) > 20 else \"\")\n",
        "            })\n",
        "\n",
        "        if compare_pk and pk_cols:\n",
        "            b_pk = distinct_pk.get(\"bronze\", 0)\n",
        "            s_pk = distinct_pk.get(\"silver\", 0)\n",
        "            if b_pk != -1 and s_pk != -1:\n",
        "                pk_drop = pct_drop(b_pk, s_pk)\n",
        "                if pk_drop > MAX_ROW_DROP_PCT:\n",
        "                    failures.append(\n",
        "                        f\"{name}: bronze->silver distinct PK drop {pk_drop:.1%} exceeds threshold {MAX_ROW_DROP_PCT:.1%}\"\n",
        "                    )\n",
        "\n",
        "    if \"silver\" in dfs and \"gold\" in dfs:\n",
        "        # Gold may be aggregated; only apply row-drop check when compare_pk is True\n",
        "        if compare_pk and pk_cols:\n",
        "            drop = pct_drop(counts[\"silver\"], counts[\"gold\"])\n",
        "            if drop > MAX_ROW_DROP_PCT:\n",
        "                failures.append(\n",
        "                    f\"{name}: silver->gold row drop {drop:.1%} exceeds threshold {MAX_ROW_DROP_PCT:.1%}\"\n",
        "                )\n",
        "\n",
        "        silver_cols = set(schemas[\"silver\"].keys())\n",
        "        gold_cols = set(schemas[\"gold\"].keys())\n",
        "\n",
        "        missing_in_gold = sorted(list(silver_cols - gold_cols))\n",
        "        if missing_in_gold:\n",
        "            results.append({\n",
        "                \"dataset\": name,\n",
        "                \"layer\": \"silver_vs_gold\",\n",
        "                \"status\": \"SCHEMA_WARN\",\n",
        "                \"detail\": f\"Missing in gold (sample): {missing_in_gold[:20]}\" + (\" ...\" if len(missing_in_gold) > 20 else \"\")\n",
        "            })\n",
        "\n",
        "    # Hard failures based on nulls/dups\n",
        "    for layer in [\"bronze\", \"silver\", \"gold\"]:\n",
        "        if layer not in dfs:\n",
        "            continue\n",
        "\n",
        "        d = dups.get(layer, 0)\n",
        "        if d == -1:\n",
        "            failures.append(f\"{name}:{layer}: missing PK columns for duplicate check\")\n",
        "        elif d > 0:\n",
        "            failures.append(f\"{name}:{layer}: duplicate PK groups={d}\")\n",
        "\n",
        "        for c, v in nulls.get(layer, {}).items():\n",
        "            if v == -1:\n",
        "                failures.append(f\"{name}:{layer}: missing required column {c}\")\n",
        "            elif v > 0:\n",
        "                failures.append(f\"{name}:{layer}: null violations {c}={v}\")\n",
        "\n",
        "print(\"Validation complete.\")\n",
        "print(f\"Datasets checked: {len(DATASETS)}\")\n",
        "print(f\"Issues found: {len(failures)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# RESULTS SUMMARY\n",
        "# =========================\n",
        "\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "rows = [Row(**r) for r in results]\n",
        "df_results = spark.createDataFrame(rows) if rows else spark.createDataFrame([], \"dataset string\")\n",
        "\n",
        "_safe_display(df_results.orderBy(F.col(\"dataset\"), F.col(\"layer\")))\n",
        "\n",
        "if failures:\n",
        "    print(\"\\nFAILURES:\")\n",
        "    for f in failures:\n",
        "        print(f\"- {f}\")\n",
        "else:\n",
        "    print(\"\\nNo failures detected.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# OPTIONAL: FAIL THE JOB\n",
        "# =========================\n",
        "\n",
        "if FAIL_PIPELINE_ON_ERROR and failures:\n",
        "    raise Exception(\"Data quality validation failed. See failures list above.\")\n",
        "\n",
        "# If you want ADF to read notebook output, you can use:\n",
        "# dbutils.notebook.exit(\"OK\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
